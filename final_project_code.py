# -*- coding: utf-8 -*-
"""Final_Project_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ybwC_usm_pXd6LT6jimPoFcWc7u9oHLr

## CS634_Data Mining - Final_Project

## Using Random Forest, GRU & SVM to Predict Diabetes

### Improting the Data
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, brier_score_loss, precision_score, balanced_accuracy_score
from sklearn.metrics import classification_report, precision_recall_curve, accuracy_score, recall_score, f1_score

path = "/content/diabetes_dataset.csv"
df = pd.read_csv(path)

display(df)

"""### Understanding the Data"""

df.info()

"""### Data Cleaning"""

df.isna().sum()

df.duplicated().sum()

"""## Data Visualization

### Histogram plot
"""

numeric = df.select_dtypes(include=np.number).columns.tolist()

for col in numeric:
    fig, ax = plt.subplots(figsize=(6,3))
    sns.histplot(df[col], kde=True, ax=ax)
    ax.set_title(f"Histogram + KDE: {col}")

"""### Histogram plot"""

n = len(numeric)
cols = 3
rows = int(np.ceil(n / cols))

fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*3))
axes = axes.flatten()

for i, col in enumerate(numeric):
    sns.histplot(df[col], kde=False, ax=axes[i])
    axes[i].set_title(col)

for j in range(i+1, len(axes)):
    axes[j].axis("off")

plt.subplots_adjust(
    left=0.05,
    right=0.98,
    top=0.95,
    bottom=0.05,
    wspace=0.35,
    hspace=0.40
)

plt.show()

"""### Violin Plot"""

fig, ax = plt.subplots(figsize=(10, max(4, 0.4*len(numeric))))
sns.violinplot(data=df[numeric], inner="quartile", orient="v", ax=ax)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
ax.set_title("Violin Plot for All Numeric Features")

"""### Heat Map"""

corr = df[numeric].corr()

fig, ax = plt.subplots(figsize=(14, 12))

sns.heatmap(
    corr,
    annot=True,
    fmt=".2f",
    cmap="vlag",
    center=0,
    linewidths=1,
    linecolor="white",
    annot_kws={"size": 8}
)

plt.xticks(rotation=45, ha="right")
plt.yticks(rotation=0)

plt.tight_layout()
plt.show()

"""### Scatterplot"""

x = numeric[0]
y = numeric[1]

fig, ax = plt.subplots(figsize=(6,4))
sns.scatterplot(data=df, x=x, y=y, hue="Outcome" if "Outcome" in df.columns else None, ax=ax)
sns.regplot(data=df, x=x, y=y, scatter=False, ax=ax, ci=None)
ax.set_title(f"{y} vs {x}")

"""### Join Plot"""

jp = sns.jointplot(data=df, x=x, y=y, kind="hex", height=6)
jp.fig.suptitle(f"Jointplot Hex: {x} vs {y}", y=1.02)

"""### Count Plot"""

categorical = df.select_dtypes(exclude=np.number).columns.tolist()

for col in categorical:
    fig, ax = plt.subplots(figsize=(6,3))
    sns.countplot(x=col, data=df, ax=ax)
    ax.set_title(f"Count Plot: {col}")

"""### Density Plot"""

numeric = df.select_dtypes(include="number").columns.tolist()

for col in numeric[:5]:
    fig, ax = plt.subplots(figsize=(6,3))
    sns.kdeplot(df[col].dropna(), ax=ax)
    ax.set_title(f"KDE of {col}")
    plt.tight_layout()
    display(fig)
    plt.close(fig)

"""### Boxplot"""

for col in numeric[:5]:
    s = df[col].dropna()
    z = (s - s.mean()) / s.std()
    outliers = s[abs(z) > 3]

    fig, ax = plt.subplots(figsize=(6,3))
    sns.boxplot(x=s, ax=ax)
    ax.set_title(f"{col} Outliers (z > 3): {len(outliers)}")

"""### Plotly Scatterplot"""

import plotly.express as px

num_cols = df.select_dtypes(include="number").columns.tolist()
dims = num_cols[:8]

sample = df.sample(n=min(len(df), 1000), random_state=0)
color = "Outcome" if "Outcome" in df.columns else None

fig = px.scatter_matrix(
    sample,
    dimensions=dims,
    color=color,
    title="Plotly Scatter Matrix",
)

fig.update_traces(diagonal_visible=True, marker=dict(size=3, opacity=0.6))

fig.update_layout(
    width=1400,
    height=1400,
    margin=dict(l=80, r=80, t=120, b=80),
    dragmode=False,
)

fig.show()

"""### Plotly Box Plot"""

import plotly.express as px

num_cols = df.select_dtypes(include="number").columns.tolist()

for col in num_cols[:3]:
    fig = px.box(
        df,
        y=col,
        points="outliers",
        title=f"Box Plot — {col}"
    )
    fig.update_layout(
        width=600,
        height=450,
        margin=dict(l=60, r=60, t=80, b=60)
    )
    fig.show()

"""### Labelling Target Column & Preprocessing the Data"""

# Manual mapping with safe fallback for unmapped values
df['diabetes_result'] = (
    df['diabetes_stage']
    .map({
        'No Diabetes': 0,
        'Pre-Diabetes': 0,
        'Type 1': 1,
        'Type 2': 1,
        'Gestational': 1
    })
    .fillna(1)  # any unknown or blank stage → diabetic (1)
    .astype(int)
)

print(df[['diabetes_stage', 'diabetes_result']].head(10))

print("\nValue counts:")
print(df['diabetes_result'].value_counts())

print(df[['diabetes_stage', 'diabetes_result']].head(10))

df[['diabetes_stage', 'diabetes_result']].value_counts()

df[['diabetes_stage','diabetes_result']].nunique()

TARGET_COL = 'diabetes_result'
assert TARGET_COL in df.columns, f"{TARGET_COL} not found in dataset columns."

# Imputing missing/zero values for medical columns
cols_zero_impute = ['glucose_fasting','glucose_postprandial','insulin_level','bmi']
present_cols = [c for c in cols_zero_impute if c in df.columns]
for col in present_cols:
    df.loc[df[col] == 0, col] = np.nan
    df[col].fillna(df[col].median(), inplace=True)

# Encoding categorical columns
cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()
for c in cat_cols:
    df[c] = df[c].astype('category').cat.codes

# Separate features and target
X = df.drop(columns=[TARGET_COL]).reset_index(drop=True)
y = df[TARGET_COL].reset_index(drop=True).astype(int)

# numeric columns
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()

print("Shape of X:", X.shape)
print("Class balance:", y.value_counts(normalize=True))

"""### Splitting Input and Output"""

corrs = df.corr()['diabetes_result'].sort_values(ascending=False)
print("Top correlated features with diabetes_result:\n")
print(corrs.head(15))

"""### Removing diabetes stage from training cause diabetes stage and diabetes result are highly correlated. which will cause the models to suffer from data leakage."""

X = df.drop(columns=['diabetes_result', 'diabetes_stage'], errors='ignore')
y = df['diabetes_result']

X,y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=23)

numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()

X_train.shape

X_test.shape

y_train.shape

y_test.shape

"""### Manual Metric Calculation"""

# Step: Manual Confusion Matrix Calculation (No libraries)

def manual_confusion_matrix(y_true, y_pred):
    """
    Manually calculate TP, TN, FP, FN without using sklearn.
    Assumes binary classification with labels 0 and 1.
    """
    # Initialize counters
    TP = TN = FP = FN = 0

    # Loop through actual and predicted values
    for actual, predicted in zip(y_true, y_pred):
        if actual == 1 and predicted == 1:
            TP += 1
        elif actual == 0 and predicted == 0:
            TN += 1
        elif actual == 0 and predicted == 1:
            FP += 1
        elif actual == 1 and predicted == 0:
            FN += 1

    print("Manual Confusion Matrix:")
    print(f"True Positives (TP): {TP}")
    print(f"True Negatives (TN): {TN}")
    print(f"False Positives (FP): {FP}")
    print(f"False Negatives (FN): {FN}")

    return TP, TN, FP, FN

"""## Model Training

### Random Forest
"""

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=23)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Metrics
cm = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Predicted probabilities for positive class
y_prob = rf_model.predict_proba(X_test)[:, 1]

# ROC AUC
auc = roc_auc_score(y_test, y_prob)
print(f"ROC AUC: {auc:.6f}")

# Brier Score (BS) and Brier Skill Score (BSS)
bs = brier_score_loss(y_test, y_prob)
p_ref = float(np.mean(y_test))
bs_ref = float(np.mean((y_test - p_ref) ** 2))
bss = 1.0 - (bs / bs_ref) if bs_ref != 0 else 0.0
print(f"Brier Score (BS): {bs:.6f}")
print(f"Brier Skill Score (BSS): {bss:.6f}")

# ROC Curve plot
fpr_vals, tpr_vals, _ = roc_curve(y_test, y_prob)
plt.plot(fpr_vals, tpr_vals, label=f'RF (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve - Random Forest')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

y_pred_rf = rf_model.predict(X_test)

# Get TP, TN, FP, FN manually
TP, TN, FP, FN = manual_confusion_matrix(y_test.tolist(), y_pred_rf.tolist())

# Scale numeric columns before cross-validation
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
X_scaled = X.copy()
X_scaled[numeric_cols] = scaler.fit_transform(X_scaled[numeric_cols])

# Define model and 10-fold CV
model = RandomForestClassifier(n_estimators=100, random_state=23)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')

# Display per-fold and average accuracy
for i, score in enumerate(cv_scores, start=1):
    print(f"Fold {i}: Accuracy = {score:.4f}")
print(f"\nAverage Accuracy across 10 folds: {np.mean(cv_scores):.4f}")

# 10-Fold Cross-Validation (per-fold + average metrics)

# Scale numeric columns
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
X_scaled = X.copy()
X_scaled[numeric_cols] = scaler.fit_transform(X_scaled[numeric_cols])

# Define model and 10-fold CV
model = RandomForestClassifier(n_estimators=100, random_state=23)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

fold_results = []

for i, (train_idx, test_idx) in enumerate(cv.split(X_scaled, y), start=1):
    X_tr, X_te = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]
    y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]

    model.fit(X_tr, y_tr)
    y_pred = model.predict(X_te)
    y_prob = model.predict_proba(X_te)[:, 1]

    # Confusion matrix
    cm = confusion_matrix(y_te, y_pred)
    TN, FP, FN, TP = cm.ravel()

    # Derived counts
    P = TP + FN
    N = TN + FP

    # Metrics
    TPR = TP / P if P > 0 else 0
    TNR = TN / N if N > 0 else 0
    FPR = FP / N if N > 0 else 0
    FNR = FN / P if P > 0 else 0
    accuracy = (TP + TN) / (P + N)
    precision = precision_score(y_te, y_pred)
    recall = recall_score(y_te, y_pred)
    f1 = f1_score(y_te, y_pred)
    bal_acc = balanced_accuracy_score(y_te, y_pred)
    error_rate = (FP + FN) / (P + N)
    TSS = TPR - FPR
    HSS_num = 2 * (TP * TN - FP * FN)
    HSS_den = ((TP + FN) * (FN + TN)) + ((TP + FP) * (FP + TN))
    HSS = HSS_num / HSS_den if HSS_den != 0 else 0

    # Brier Score (BS) and Brier Skill Score (BSS)
    BS = brier_score_loss(y_te, y_prob)
    p_ref = np.mean(y_te)
    BS_ref = np.mean((y_te - p_ref) ** 2)
    BSS = 1 - (BS / BS_ref) if BS_ref != 0 else 0

    # AUC
    AUC = roc_auc_score(y_te, y_prob)

    fold_results.append({
        'Fold': i, 'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN,
        'P': P, 'N': N,
        'TPR': TPR, 'TNR': TNR, 'FPR': FPR, 'FNR': FNR,
        'Accuracy': accuracy, 'Balanced Accuracy': bal_acc,
        'Precision': precision, 'Recall': recall, 'F1': f1,
        'Error Rate': error_rate, 'TSS': TSS, 'HSS': HSS,
        'Brier Score': BS, 'Brier Skill Score': BSS, 'AUC': AUC
    })

# Create per-fold results DataFrame
results_df = pd.DataFrame(fold_results)
display(results_df)

# Compute and display average metrics
avg_row = results_df.mean(numeric_only=True).to_frame().T
avg_row.index = ["Average"]
display(avg_row)

"""### LSTM"""

# Reshape data for LSTM [samples, timesteps, features]
X_train_lstm = np.expand_dims(X_train.values, axis=1)
X_test_lstm = np.expand_dims(X_test.values, axis=1)

# Build LSTM model
lstm_model = Sequential([
    LSTM(64, activation='relu', input_shape=(1, X_train.shape[1])),
    Dense(1, activation='sigmoid')
])

lstm_model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])
history = lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test), verbose=1)

# Evaluate
loss, accuracy = lstm_model.evaluate(X_test_lstm, y_test)
print(f"LSTM Accuracy: {accuracy:.4f}")

# Predictions & Metrics
y_prob_lstm = lstm_model.predict(X_test_lstm)          # probabilities for ROC
y_pred_lstm = (y_prob_lstm > 0.5).astype(int)          # thresholded predictions

# Confusion Matrix and Classification Report
cm = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Predicted probabilities for positive class
y_prob = rf_model.predict_proba(X_test)[:, 1]

# ROC AUC
auc = roc_auc_score(y_test, y_prob)
print(f"ROC AUC: {auc:.6f}")

# Brier Score (BS) and Brier Skill Score (BSS)
bs = brier_score_loss(y_test, y_prob)
p_ref = float(np.mean(y_test))
bs_ref = float(np.mean((y_test - p_ref) ** 2))
bss = 1.0 - (bs / bs_ref) if bs_ref != 0 else 0.0
print(f"Brier Score (BS): {bs:.6f}")
print(f"Brier Skill Score (BSS): {bss:.6f}")

# ROC Curve plot
fpr_vals, tpr_vals, _ = roc_curve(y_test, y_prob)
plt.plot(fpr_vals, tpr_vals, label=f'RF (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve - Random Forest')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

y_pred_rf = rf_model.predict(X_test)

# Get TP, TN, FP, FN manually
TP, TN, FP, FN = manual_confusion_matrix(y_test.tolist(), y_pred_rf.tolist())

# Scale numeric columns before cross-validation
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
X_scaled = X.copy()
X_scaled[numeric_cols] = scaler.fit_transform(X_scaled[numeric_cols])

# model and 10-fold CV
model = RandomForestClassifier(n_estimators=100, random_state=23)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# cross-validation
cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')

# per-fold and average accuracy
for i, score in enumerate(cv_scores, start=1):
    print(f"Fold {i}: Accuracy = {score:.4f}")
print(f"\nAverage Accuracy across 10 folds: {np.mean(cv_scores):.4f}")

# 10-Fold Cross-Validation with LSTM (per-fold metrics + average)

np.random.seed(42)
tf.random.set_seed(42)
n_splits = 10
epochs = 8
batch_size = 32
lstm_units = 32
l2_reg = 1e-4
dropout_rate = 0.2

# Scale numeric columns globally then split
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
X_scaled = X.copy()
scaler_global = StandardScaler()
if len(numeric_cols) > 0:
    X_scaled[numeric_cols] = scaler_global.fit_transform(X_scaled[numeric_cols])

skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

fold_results = []
fold_idx = 0

for train_idx, test_idx in skf.split(X_scaled, y):
    fold_idx += 1
    # Split
    X_tr_df = X_scaled.iloc[train_idx].reset_index(drop=True)
    X_te_df = X_scaled.iloc[test_idx].reset_index(drop=True)
    y_tr = y.iloc[train_idx].values
    y_te = y.iloc[test_idx].values

    # Convert to numpy and reshape for LSTM
    X_tr = X_tr_df.values.astype(np.float32)
    X_te = X_te_df.values.astype(np.float32)
    X_tr_lstm = np.expand_dims(X_tr, axis=1)
    X_te_lstm = np.expand_dims(X_te, axis=1)

    # Build a fresh LSTM model for each fold
    model = Sequential([
        LSTM(lstm_units, activation='tanh', kernel_regularizer=l2(l2_reg), input_shape=(1, X_tr.shape[1])),
        Dropout(dropout_rate),
        Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_reg))
    ])
    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])

    # Training
    model.fit(X_tr_lstm, y_tr, epochs=epochs, batch_size=batch_size, validation_data=(X_te_lstm, y_te), verbose=0)

    # Predict
    y_prob = model.predict(X_te_lstm, batch_size=batch_size, verbose=0).ravel()
    y_pred = (y_prob > 0.5).astype(int)

    # Confusion matrix
    cm = confusion_matrix(y_te, y_pred)
    TN, FP, FN, TP = cm.ravel()

    P = TP + FN
    N = TN + FP
    total = P + N

    # Rates and basic metrics
    def sdiv(a, b): return float(a) / float(b) if float(b) != 0 else 0.0

    TPR = sdiv(TP, P)
    TNR = sdiv(TN, N)
    FPR = sdiv(FP, N)
    FNR = sdiv(FN, P)
    Accuracy = sdiv(TP + TN, total)
    Precision = sdiv(TP, TP + FP)
    Recall = TPR
    F1 = sdiv(2 * Precision * Recall, (Precision + Recall)) if (Precision + Recall) != 0 else 0.0
    Balanced_Accuracy = 0.5 * (TPR + TNR)
    Error_rate = sdiv(FP + FN, total)
    TSS = TPR - FPR
    numerator_hss = 2.0 * (TP * TN - FP * FN)
    denom_hss = ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN))
    HSS = sdiv(numerator_hss, denom_hss)

    # Probabilistic metrics
    BS = float(brier_score_loss(y_te, y_prob))
    p_ref = float(np.mean(y_te))
    BS_ref = float(np.mean((y_te - p_ref) ** 2))
    BSS = 1.0 - (BS / BS_ref) if BS_ref != 0 else 0.0

    # AUC (per-fold)
    try:
        AUC = float(roc_auc_score(y_te, y_prob)) if len(np.unique(y_te)) > 1 else None
    except Exception:
        AUC = None

    fold_results.append({
        'Fold': fold_idx,
        'TP': int(TP), 'TN': int(TN), 'FP': int(FP), 'FN': int(FN),
        'P': int(P), 'N': int(N),
        'TPR': round(TPR, 4), 'TNR': round(TNR, 4), 'FPR': round(FPR, 4), 'FNR': round(FNR, 4),
        'Accuracy': round(Accuracy, 4), 'Balanced Accuracy': round(Balanced_Accuracy, 4),
        'Precision': round(Precision, 4), 'Recall': round(Recall, 4), 'F1': round(F1, 4),
        'Error Rate': round(Error_rate, 4), 'TSS': round(TSS, 4), 'HSS': round(HSS, 4),
        'Brier Score': round(BS, 6), 'BSS': round(BSS, 6), 'AUC': round(AUC, 6) if AUC is not None else None
    })

# results DataFrame
results_df = pd.DataFrame(fold_results).set_index('Fold')
pd.set_option('display.precision', 4)
display(results_df)

# Average row
avg_row = results_df.mean(numeric_only=True).to_frame().T
avg_row.index = ['Average']
display(avg_row)

"""### KNN"""

# Scale numeric columns
scaler_knn = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()
X_train_scaled[numeric_cols] = scaler_knn.fit_transform(X_train_scaled[numeric_cols])
X_test_scaled[numeric_cols] = scaler_knn.transform(X_test_scaled[numeric_cols])

# KNN model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Predictions
y_pred_knn = knn.predict(X_test_scaled)
y_prob_knn = knn.predict_proba(X_test_scaled)[:, 1]

# Metrics
cm = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Predicted probabilities for positive class
y_prob = rf_model.predict_proba(X_test)[:, 1]

# ROC AUC
auc = roc_auc_score(y_test, y_prob)
print(f"ROC AUC: {auc:.6f}")

# Brier Score (BS) and Brier Skill Score (BSS)
bs = brier_score_loss(y_test, y_prob)
p_ref = float(np.mean(y_test))
bs_ref = float(np.mean((y_test - p_ref) ** 2))
bss = 1.0 - (bs / bs_ref) if bs_ref != 0 else 0.0
print(f"Brier Score (BS): {bs:.6f}")
print(f"Brier Skill Score (BSS): {bss:.6f}")

# ROC Curve plot
fpr_vals, tpr_vals, _ = roc_curve(y_test, y_prob)
plt.plot(fpr_vals, tpr_vals, label=f'RF (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve - Random Forest')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

y_pred_rf = rf_model.predict(X_test)

# Get TP, TN, FP, FN manually
TP, TN, FP, FN = manual_confusion_matrix(y_test.tolist(), y_pred_rf.tolist())

# Scaling numeric columns before cross-validation
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
X_scaled = X.copy()
X_scaled[numeric_cols] = scaler.fit_transform(X_scaled[numeric_cols])

# model and 10-fold CV
model = RandomForestClassifier(n_estimators=100, random_state=23)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# cross-validation
cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')

# per-fold and average accuracy
for i, score in enumerate(cv_scores, start=1):
    print(f"Fold {i}: Accuracy = {score:.4f}")
print(f"\nAverage Accuracy across 10 folds: {np.mean(cv_scores):.4f}")

# 10-Fold Cross-Validation for KNN (per-fold metrics + average)

n_splits = 10
n_neighbors = 25           # regularized choice
weights = 'uniform'        # 'uniform' or 'distance'
p = 2                      # Euclidean
random_state = 42

# Prepare numeric columns reference
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()

skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
fold_results = []

def sdiv(a, b): return float(a) / float(b) if float(b) != 0 else 0.0

for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):
    # Split
    X_tr, X_te = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()
    y_tr, y_te = y.iloc[train_idx].values, y.iloc[test_idx].values

    # Scale numeric features
    scaler = StandardScaler()
    if len(numeric_cols) > 0:
        X_tr[numeric_cols] = scaler.fit_transform(X_tr[numeric_cols])
        X_te[numeric_cols] = scaler.transform(X_te[numeric_cols])

    # Train KNN
    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, p=p, n_jobs=-1)
    knn.fit(X_tr, y_tr)

    # Predictions
    y_pred = knn.predict(X_te)
    y_prob = knn.predict_proba(X_te)[:, 1] if hasattr(knn, "predict_proba") else None

    # Confusion matrix
    cm = confusion_matrix(y_te, y_pred)
    TN, FP, FN, TP = cm.ravel()

    P = TP + FN
    N = TN + FP
    total = P + N

    # Rates & metrics
    TPR = sdiv(TP, P)
    TNR = sdiv(TN, N)
    FPR = sdiv(FP, N)
    FNR = sdiv(FN, P)
    Accuracy = sdiv(TP + TN, total)
    Precision = sdiv(TP, TP + FP)
    Recall = TPR
    F1 = sdiv(2 * Precision * Recall, (Precision + Recall)) if (Precision + Recall) != 0 else 0.0
    Balanced_Accuracy = 0.5 * (TPR + TNR)
    Error_rate = sdiv(FP + FN, total)
    TSS = TPR - FPR
    numerator_hss = 2.0 * (TP * TN - FP * FN)
    denom_hss = ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN))
    HSS = sdiv(numerator_hss, denom_hss)

    # Probabilistic metrics
    if y_prob is not None:
        BS = float(brier_score_loss(y_te, y_prob))
        p_ref = float(np.mean(y_te))
        BS_ref = float(np.mean((y_te - p_ref) ** 2))
        BSS = 1.0 - (BS / BS_ref) if BS_ref != 0 else 0.0
        try:
            AUC = float(roc_auc_score(y_te, y_prob)) if len(np.unique(y_te)) > 1 else None
        except Exception:
            AUC = None
    else:
        BS = float(np.mean((y_te - y_pred) ** 2))
        p_ref = float(np.mean(y_te))
        BS_ref = float(np.mean((y_te - p_ref) ** 2))
        BSS = 1.0 - (BS / BS_ref) if BS_ref != 0 else 0.0
        AUC = None

    fold_results.append({
        'Fold': fold_idx,
        'TP': int(TP), 'TN': int(TN), 'FP': int(FP), 'FN': int(FN),
        'P': int(P), 'N': int(N),
        'TPR': round(TPR, 4), 'TNR': round(TNR, 4), 'FPR': round(FPR, 4), 'FNR': round(FNR, 4),
        'Accuracy': round(Accuracy, 4), 'Balanced_Accuracy': round(Balanced_Accuracy, 4),
        'Precision': round(Precision, 4), 'Recall': round(Recall, 4), 'F1': round(F1, 4),
        'Error_Rate': round(Error_rate, 4), 'TSS': round(TSS, 4), 'HSS': round(HSS, 4),
        'Brier_Score': round(BS, 6), 'BSS': round(BSS, 6), 'AUC': round(AUC, 6) if AUC is not None else None
    })

results_df = pd.DataFrame(fold_results).set_index('Fold')
pd.set_option('display.precision', 4)
display(results_df)

avg_row = results_df.mean(numeric_only=True).to_frame().T
avg_row.index = ['Average']
display(avg_row)

#